---
title: "12. 데이터 전처리 이해와 실무_데이터 축소-특징 선택"
date: "2025-03-24"
link: "https://velog.io/@ehekaanldk/12.-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC-%EC%9D%B4%ED%95%B4%EC%99%80-%EC%8B%A4%EB%AC%B4%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%B6%95%EC%86%8C-%ED%8A%B9%EC%A7%95-%EC%84%A0%ED%83%9D"
series: "Uncategorized"
---

<p>데이터 축소를 위한 특징 선택에 대해서 알아보고 특징 선택 방안이 어떤 것들이 있는지 알아보자.</p>
<h3 id="특징-선택-정의">특징 선택 정의</h3>
<h3 id="feature-selection">feature selection</h3>
<p>가장 좋은 성능을 보여줄 수 있는 데이터의 부분 집합(subset)을 찾아내는 방법
모델 생성에 밀접한 데이터의 부분 집합을 선택하여 연산 효율성 및 모델 성능을 확보한다. 
feature engineering의 주요 토픽이며 데이터 분석에서 많이 사용된다. </p>
<p>목적 및 필요성)
<img alt="" src="https://velog.velcdn.com/images/ehekaanldk/post/803a68ef-538d-485f-8ba5-7fad3686cab9/image.png" />
원본 데이터의 존재하지 않는 데이터를 생성하여 대체하는 것(툭징 생성)이 아니라 주어진 원본 데이터에서 좋은 특징을 골라내는 것이다. </p>
<h3 id="특징-선택-방안">특징 선택 방안</h3>
<ul>
<li>필터(Filter) : 특징들에 대해서 통계적 점수를 부여하여 순위를 매겨 선택한다. <ul>
<li>실행 속도가 빠르다</li>
<li>도메인 지식기반의 데이터 활용 여부 파악</li>
</ul>
</li>
<li>래퍼(wrapper) : 특징들의 모든 조합을 알고리즘에 반복적으로 적용하여 특징 선택한다.<ul>
<li>특징 조합을 찾는 과정이 오래걸림</li>
<li>시간과 비용이 크게 발생</li>
</ul>
</li>
<li>임베디드(Embedded) : 모델 정확도에 기여하는 특징들을 선택한다.<ul>
<li>필터와 래퍼의 결합</li>
<li>모델 자체의 변수 선택을 할 수 있는 알고리즘</li>
</ul>
</li>
</ul>
<h4 id="filter-">Filter ()</h4>
<p>특징들에 대한 통계적 기법 기반의 점수 및 순위를 부여하여 선택한다. </p>
<ul>
<li>카이제곱 필터 chi-square filter : 범주형인 독립 및 종속 변수 간의 유의미성을 도출한다.<ul>
<li>가장 높은 순위 2개의 feature선택</li>
<li>연속형 변수를 이산화(범주)를 하여 활용 가능</li>
</ul>
</li>
<li>상관관계 필터 correlation filter : 연속형인 독립 및 종속변수 간의 유의미성을 도출한다. <ul>
<li>임계치를 설정하여 이상의 feature선택</li>
</ul>
</li>
</ul>
<h4 id="wrapper-반복적-특징-조합-탐색">Wrapper (반복적 특징 조합 탐색)</h4>
<p>원본 데이터 내 변수 간 조합을 탐색하여 특징을 선택한다. 최적의 부분 데이터 집합(subset)을 도출하는 방법론으로 기존 데이터에서 테스트를 위한 셋을 만들어야 한다. 
<img alt="" src="https://velog.velcdn.com/images/ehekaanldk/post/d7b31e78-38b6-4568-ac2b-abcb49b7b714/image.png" /></p>
<ul>
<li>변수가 없는 방식에서 추가해내는 방법</li>
<li>전체에서 필요없는 변수를 제거해내는 방법 : 재귀적 특성 제거(Recursive feature elimination)</li>
</ul>
<h4 id="embedded-모델-기반-특징-선택">Embedded (모델 기반 특징 선택)</h4>
<p>모델을 학습하여 정확도에 기여하는 특징을 선택하는 방안이다. 알고리즘 자체에 특징을 선택하는 방식이 내장되어 모델 성능에 기여하는 특징을 도출해낸다. 
<img alt="" src="https://velog.velcdn.com/images/ehekaanldk/post/ff90e13e-e50d-451c-b542-9445acb3e5b2/image.png" /></p>
<ul>
<li>트리 계열 모델 기반 : feature importance, 랜던 포레스트
랜덤포레스트 모형 기반의 알고리즘은 대표적으로 보루타 알고리즘이 존재한다. </li>
</ul>
<p><strong>보루타 알고리즘(boruta algorithm)</strong>
기존 데이터를 복원추출해서 만든 랜덤 변수(shadow)보다 모형 생성이 영향을 주지 못하는 경우에 가치가 낮다고 인식하여 제거하는 알고리즘이다. 
원본 데이터를 복제하고 임의의 값을 추가하여 만든 임의의 변수로 기존 특징과의 중요도를 판단하기 위한 참조값이 된다. 랜덤값과 다름이 없음으로 랜덤 변수의 중요도 보다 낮은 기존의 특징은 중요하지 않을 것이다.
<img alt="" src="https://velog.velcdn.com/images/ehekaanldk/post/f89e6507-e17e-4765-ad20-d7534d40afc9/image.png" /></p>
<h4 id="실습">실습</h4>
<p>데이터 : breasr cancer wisconsin</p>
<p>모델을 학습하고 성능을 확인하기 위해서는 훈련 셋과 테스트 셋으로 나눈다. </p>
<p>from sklearn.moel_selection import train_test_split 을 사용하면 8:2로 쉽게 나눌 수 있다. </p>
<p>train 데이터에서 target과 input 히스토그램으로 시각화한다. target인 악성과 양성에 따라 분포를 확인해본다. 독립변수별로 target 간의 분포를 확인할 수 있다. 시각화를 기반으로 빠르게 확인하고 모델링을 진행할 수 있다. </p>
<pre><code>import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(10, 3, figsjze=(20, 40))
axes = axes.flatten()

for i in range(30):
    sns.histplot(data=train_df, x= train_df.columns[i], hue=&quot;diagnosis&quot;, multiple=&quot;layer&quot;, ax=axes[i])

plt.show()</code></pre><p>시각화 기반 특징들의 유의미함 예상한다.</p>
<p>유의할 것이라고 예상되는 변수
. radius_mean, perimeter_mean, area_mean, concave_point_mean, radius_worst, perimeter_worst, area_worst, concave_point_worst 등</p>
<p>유의하지 않을 것이라 예상되는 변수
. smoothness_mean, symmetry_mean, fractal_dimension_mean, texture_se, smoothness_se, symmetry_se, fractal_dimension_se 등</p>
<hr />
<p>RFE 기반의 주요 특징 선택(wrapper)</p>
<p>SVM 기반의 RFE 방식을 사용한다. 이는 특징들의 스케일에 민감한 SVM의 특징에 따라 변수의 scaling을 따로 진행한다. </p>
<pre><code># SVM 기반 RFE 수행|
from sklearn.preprocessing import StandardScaler
# 스케일에 민감한 SVM의 특징에 따라 변수의 scaling을 따로 진행

scaler = StandardScaler()
scaler.fit(X_train)
scale_X_train = scaler.transform(X_train)</code></pre><p>유의미한 특징 5개만 가져오도록 한다. 1이라고 나온 컬럼들이 유의미한 5개의 특징이다. </p>
<pre><code># RFE 를 적용할 모델 SVM 지정
estimator_mdl = SVC(kernel=&quot;linear&quot;)
# SVM 학습 기반의 RFE 실행 및 유의미한 개수 지정 : 5개
svm_rfe = RFE(estimator = estimator_mdl, n_features_to_select= 5I

svm_rfe_rst = svm_rfe.fit(scale_X_train, y_train.values.ravel())
svm_rfe_rst.ranking_
# 1로 나온 변수가 최종 유의미한 특징으로 도출된 컬럼</code></pre><p>train과 test를 동일한 기준으로 스케일링을 적용해야 한다. </p>
<pre><code># 도출한 특짐 조함으로 test 진행
scale_X_test = scaler.transform(X_test)
prediction = pd.DataFrame(svm_rfe.predict(scale_X_test), columns = ['pred_rst'])
prediction</code></pre><p>성능을 평가하기 위해 데이터를 수치형으로 변경한다. </p>
<pre><code># 타겟 클러스 데이터 수치형 변경
y_test ['diagnosis'] = y_test ['diagnosis']. replace('M', 1)
y_test ['diagnosis'] = y_test ['diagnosis'].replace('B', 0)
prediction['pred_rst'] = prediction['pred_rst'].replace('M', 1)|
prediction['pred_rst'] = prediction['pred_rst'].replace('B', 0)</code></pre><p>accuracy를 기준으로 성능을 평가한다. </p>
<pre><code># 결과 확인
# accuracy
from sklearn.metrics import accuracy_score
print ('accuracy: ', round(accuracy_score(y_test['diagnosis'], prediction['pred_rst']).5))
# auc
from sklearn.metrics import roc_auc_score
print ('auc: ',round(roc_auc_score(y_test['diagnosis'], prediction['pred_rst']), 5))</code></pre><p>최종 선택된 5개의 특징 컬럼이 어떤 컬럼인지 확인한다. 1로 나타냈던 컬럼을 확인하면 된다. </p>
<pre><code># 최종 선택된 5개의 특징 컬럼이 어떠한 컬럼인지 확인
chk_var=[]
for name, svm_rfe_rst.ranking_ in zip(X_train.columns, svm_rfe_rst.ranking ): # 같이 묶어서 호출
    Ist_chk= [name, svm_rfe_rst.ranking_]
    chk_var.append(Ist_chk)

chk_svm_rfe = pd.DataFrame(chk_var,columns = ['feature_names', 'svm_rfe_feature'])
chk_svm_rfe = chk_svm_rfe[chk_svm_rfe[&quot;svm_rfe_feature&quot;] == 1]
chk_svm_rfe</code></pre><p>SVM-RFE 기반 판단</p>
<p>• 위 특징 중 radius_worst/ area_worst/ concave_point_worst 등 3, 특징이 포함됨
• SVM 이외에도 decision tree, logistic regression 등의 알고리즘을 자유롭게 활용하여 RFE 가능
• RFE 내에서 사용된 모텔이 어떤 특징을 선택하고 예측 문제에 대한 성능을 결정하는 데 중요한 차이를 만들 수 있음</p>
<hr />
<p>랜덤포레스트 기반의 embedded 특징 선택</p>
<pre><code>#RF 모형 기반 특정 선택|
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier</code></pre><p>랜덤포레스트 기반의 특징 선택은 해당 나무의 개수가 많아질수록 좋은 성능을 기대할 수 있지만 연산량이 많아져 비례적으로 학습 시간은 길어진다. </p>
<pre><code># RF 모형 생성
embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state= 1))
embeded_rf_selector.fit(scale_X_train, y_train.values.ravel())

# RF 기반 Embedded 특징 선택 결과
embeded_rf_support = embeded_rf_selector.get_support()
embeded_rf_feature = X_train. loc[:,embeded_rf_support].columns.tolist()
print(str(len(embeded_rf_feature)). 'selected features&quot;)</code></pre><p>시각화 기반 유의할 것이라고 예상되었던 특징
. radius_mean, perimeter_mean, area_mean, concave_point_mean, radius_worst, perimeter_worst, area_worst, concave_point_worst 등</p>
<p>Embedded (Random Forest) 기반 판단
. 91 58 8 radius_mean/ perimeter_mean/ area_mean/ concave points_mean/ radius_worst/ perimeter_worst/ area_worst/ concave points_worst 등 8개</p>
<hr />
<p>Boruta 알고리즘</p>
<p>• Boruta Algorithm 은 랜덤포레스트 모형 기반 특징 선택 알고리즘
• 기존 데이터를 임의로 복제하여 랜덤변수(Shadow 변수)를 생성하고 그 보다 낮은 중요도를 지닌 특징을 제외함</p>
<pre><code>from boruta import BorutaPy

# boruta 알고리증은 랜덤포레스트 모형 기반이므로 먼저 RF 모형 설정
rf = RandomForestClassifier(random_state = 1)</code></pre><p>auto로 설정하면 데이터셋 크기에 자동으로 설정된다. </p>
<pre><code># boruta 기반 특정 선택
boruta_selector= BorutaPy(rf. n_estimators=&quot;auto&quot;, random_state i)I

# 'auto' 인 경우는 자동으로 데이터셋 사이즈료 고려하여 자동 설정

boruta_selector.fit(scale_X_train, y_train.values.ravel())
boruta_selector.ranking_</code></pre><p>1로된 특징이 선택된 특징이다. </p>
<pre><code># 최종 선택된 특징 컬럼이 어떠한 컬럼인지 학인
chk_var_boruta=[]
for name, boruta_selector.ranking_ iㅜ zip(X_train.columns, boruta_selector.ranking_ ): # 같이 묶어서 호출
Ist_chk= [name, boruta_selector.ranking_]
chk_var_boruta.append(lst_chk)

chk_boruta = pd.DataFrame(chk_var_boruta.columns = ['feature_names&quot;, &quot;boruta_feature'])
chk_boruta = chk_boruta[chk_boruta[&quot;boruta_feature&quot;] == 1]
chk_boruta</code></pre><p>시각화 기반 유의하지 않을 것이라 예상되었던 특징
. smoothness_mean, symmetry_mean, fractal_dimension_mean, texture_se, smoothness_se, symmetry_se, fractal_dimension_se 등</p>
<p>Boruta 기반 판단 시
· symmetry_mean/fractal_dimension_mean/texture_se/smoothness_se/ symmetry_se/fractal_dimension_se 등 6개 특징 포함됨</p>
<hr />
<p>정리)
Filter 방식
• 데이터 유형 별 통계적 기법 기반의 점수 및 순위 부여하여 주요 특징 선택</p>
<p>Wrapper 방식
• 최적의 데이터 조합을 찾기 위한 변수 추가/삭제를 반복적으로 수행하여 특징 선택</p>
<p>Embedded 방식
• 모델 기반의 학습 성능에 기여하는 특징 선택</p>
<ol start="12">
<li>20</li>
</ol>
